{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.experimental    import enable_iterative_imputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.linear_model    import Lasso, Ridge\n",
    "from sklearn.ensemble        import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from   sklearn.metrics         import mean_squared_error,mean_absolute_error\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricky Zhang / MSDS 699 Final Project: Zillowâ€™s Home Value Prediction: Zestimate\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question / Hypothesis\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The research question of this project is to predict the log-error between Zillow's estimate and the actual sale price, given all the features of a home.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obviously, the housing price is highly dependent on some of the most prominent features such as SQFT, # of bedrooms, # of bathrooms, etc. However, we still have numerous other interesting features of homes in this dataset, like AC type, garage SQFT, fire place count. I'm very excited to find out how they are correlated with the price__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data source: https://www.kaggle.com/c/zillow-prize-1/data?select=train_2017.csv \n",
    "### https://www.kaggle.com/c/zillow-prize-1/data?select=properties_2017.csv\n",
    "df = pd.read_csv('zillow.csv')\n",
    "### the dataframe I'm working on is downsampled to 3,000 rows for computation efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('logerror',axis=1)\n",
    "y = df['logerror'].copy() ###log_error is the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "## use cv on train set to choose model, final test on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [] ###for columns has more than half missing values, I simply get rid of them\n",
    "for col in df.columns.tolist():\n",
    "    if df[col].isnull().sum() / df['parcelid'].count() > 0.5:\n",
    "        drop_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 32 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   parcelid                      3000 non-null   int64  \n",
      " 1   airconditioningtypeid         3000 non-null   float64\n",
      " 2   bathroomcnt                   3000 non-null   float64\n",
      " 3   bedroomcnt                    3000 non-null   float64\n",
      " 4   buildingqualitytypeid         2635 non-null   float64\n",
      " 5   calculatedbathnbr             2998 non-null   float64\n",
      " 6   calculatedfinishedsquarefeet  3000 non-null   float64\n",
      " 7   finishedsquarefeet12          2961 non-null   float64\n",
      " 8   fips                          3000 non-null   float64\n",
      " 9   fullbathcnt                   2998 non-null   float64\n",
      " 10  heatingorsystemtypeid         2819 non-null   float64\n",
      " 11  latitude                      3000 non-null   float64\n",
      " 12  longitude                     3000 non-null   float64\n",
      " 13  lotsizesquarefeet             2786 non-null   float64\n",
      " 14  propertycountylandusecode     3000 non-null   object \n",
      " 15  propertylandusetypeid         3000 non-null   float64\n",
      " 16  propertyzoningdesc            2676 non-null   object \n",
      " 17  rawcensustractandblock        3000 non-null   float64\n",
      " 18  regionidcity                  2920 non-null   float64\n",
      " 19  regionidcounty                3000 non-null   float64\n",
      " 20  regionidzip                   2995 non-null   float64\n",
      " 21  roomcnt                       3000 non-null   float64\n",
      " 22  unitcnt                       2671 non-null   float64\n",
      " 23  yearbuilt                     3000 non-null   float64\n",
      " 24  structuretaxvaluedollarcnt    2999 non-null   float64\n",
      " 25  taxvaluedollarcnt             3000 non-null   float64\n",
      " 26  assessmentyear                3000 non-null   float64\n",
      " 27  landtaxvaluedollarcnt         3000 non-null   float64\n",
      " 28  taxamount                     3000 non-null   float64\n",
      " 29  censustractandblock           2986 non-null   float64\n",
      " 30  logerror                      3000 non-null   float64\n",
      " 31  transactiondate               3000 non-null   object \n",
      "dtypes: float64(28), int64(1), object(3)\n",
      "memory usage: 750.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop(drop_cols,axis=1).info() ###take another look after dropping these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols.append('transactiondate') ##get rid of transaction date too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Though the fact that only 3 columns are type object from df.info(), there're actually more categoricals already\n",
    "### in numeric form. We would also want to one-hot encode these to pass into linear models\n",
    "cat = ['airconditioningtypeid','buildingqualitytypeid','heatingorsystemtypeid','propertyzoningdesc',\n",
    "       'propertycountylandusecode','propertylandusetypeid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remaining are continuous varibles\n",
    "remaining = [x for x in all_cols if x not in cat and x not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
    "                     ('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True))])\n",
    "\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(missing_values=np.nan,strategy='median', add_indicator=True)),\n",
    "                     ('scaler', StandardScaler())\n",
    "                      ])\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat),\n",
    "                                   ('continuous', con_pipe, remaining),\n",
    "                                   ('drop', 'drop', drop_cols)\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms & Search\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology: grid search for optimal hyperparameters for Lasso & Ridge to get a baseline model, then fit a RF model, compare two models performance to decide final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "###use two linear algorithms as baseline models\n",
    "linear_algorithms = {\n",
    "    \n",
    "    Lasso:{'alpha':(5, 0.5, 0.05, 0.005, 0.0005, 1, 0.1, 0.01,0.001,0.0001),\n",
    "           'normalize':(True,False)},\n",
    "    \n",
    "    Ridge:{'normalize':(True,False),\n",
    "           'alpha':(5, 0.5, 0.05, 0.005, 0.0005, 1, 0.1, 0.01,0.001,0.0001)}\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###find optimal hyperparameters for RF and compare to baseline linear model\n",
    "rf = {\n",
    "    \n",
    "    RandomForestRegressor:{'n_estimators':np.arange(10,500,20), ##with more trees, bagged trees see more of training data\n",
    "                            'criterion' : (\"mse\", \"mae\"), \n",
    "                            'max_depth':np.arange(5,100,5),\n",
    "                            'min_samples_split':np.arange(1,12,2),\n",
    "                            'min_samples_leaf':np.arange(1,50,1),\n",
    "                            'max_features' : (\"auto\", \"sqrt\", \"log2\")##all these 5 hp limit overfitting for each tree,\n",
    "                                                                     ##therefore increasing generality\n",
    "                            }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03298179255704525, tolerance: 0.0037698047637306008\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010035615830695122, tolerance: 0.0038055665482468032\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01405760581299731, tolerance: 0.0038230319177292196\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.17570600196489394, tolerance: 0.0033831859627253616\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.010402265545302214, tolerance: 0.003364712498700356\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43206900553430927, tolerance: 0.0037698047637306008\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:513: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14835097698229305, tolerance: 0.0038055665482468032\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "###Grid search CV to tune hp for linear model\n",
    "for algo,hp in linear_algorithms.items():\n",
    "    pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                     ('rgr',     algo())])\n",
    "    \n",
    "    d=dict()\n",
    "    \n",
    "    for k,v in hp.items():\n",
    "        s = 'rgr__'+k\n",
    "        d[s] = v\n",
    "        \n",
    "    linear_rgr_grid_cv = GridSearchCV(estimator=pipe, \n",
    "                              param_grid=d,\n",
    "                              cv=5,\n",
    "                              verbose=True)  \n",
    "    linear_rgr_grid_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [-0.00233394  0.00415299  0.00475883         nan -0.00057651  0.00080045\n",
      "         nan  0.00063649         nan  0.00097041]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##RF takes much more time to train, so we use RandomizedSearchCV here\n",
    "for algo,hp in rf.items():\n",
    "    pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                     ('rgr',     algo())])\n",
    "    \n",
    "    d=dict()\n",
    "    \n",
    "    for k,v in hp.items():\n",
    "        s = 'rgr__'+k\n",
    "        d[s] = v\n",
    "        \n",
    "    rf_rgr_rand_cv = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=d,\n",
    "                              cv=5,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True)  \n",
    "    rf_rgr_rand_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='most_frequent'))]),\n",
       "                                                  ['airconditioningtypeid',\n",
       "                                                   'buildingqualitytypeid',\n",
       "                                                   'heatingorsystemtypeid',\n",
       "                                                   'propertyzoningdesc',\n",
       "                                                   'propertycountylandusecode',\n",
       "                                                   'propertylan...\n",
       "                                                   'garagetotalsqft',\n",
       "                                                   'hashottuborspa', 'poolcnt',\n",
       "                                                   'poolsizesum',\n",
       "                                                   'pooltypeid10',\n",
       "                                                   'pooltypeid2', 'pooltypeid7',\n",
       "                                                   'regionidneighborhood',\n",
       "                                                   'storytypeid',\n",
       "                                                   'threequarterbathnbr',\n",
       "                                                   'typeconstructiontypeid',\n",
       "                                                   'yardbuildingsqft17',\n",
       "                                                   'yardbuildingsqft26',\n",
       "                                                   'numberofstories',\n",
       "                                                   'fireplaceflag',\n",
       "                                                   'taxdelinquencyflag',\n",
       "                                                   'taxdelinquencyyear',\n",
       "                                                   'transactiondate'])])),\n",
       "                ('rgr', Ridge(alpha=5, normalize=True))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_rgr_grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='most_frequent'))]),\n",
       "                                                  ['airconditioningtypeid',\n",
       "                                                   'buildingqualitytypeid',\n",
       "                                                   'heatingorsystemtypeid',\n",
       "                                                   'propertyzoningdesc',\n",
       "                                                   'propertycountylandusecode',\n",
       "                                                   'propertylan...\n",
       "                                                   'pooltypeid2', 'pooltypeid7',\n",
       "                                                   'regionidneighborhood',\n",
       "                                                   'storytypeid',\n",
       "                                                   'threequarterbathnbr',\n",
       "                                                   'typeconstructiontypeid',\n",
       "                                                   'yardbuildingsqft17',\n",
       "                                                   'yardbuildingsqft26',\n",
       "                                                   'numberofstories',\n",
       "                                                   'fireplaceflag',\n",
       "                                                   'taxdelinquencyflag',\n",
       "                                                   'taxdelinquencyyear',\n",
       "                                                   'transactiondate'])])),\n",
       "                ('rgr',\n",
       "                 RandomForestRegressor(max_depth=50, max_features='sqrt',\n",
       "                                       min_samples_leaf=14, min_samples_split=7,\n",
       "                                       n_estimators=450))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rgr_rand_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use best estimator from linear model and RF to perform CV on training set; we use both MSE & MAE to evaluate performance, because they both measure how off our prediction is from true response, but MSE is more sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 5-Fold mae: 0.06300257554302456\n",
      "Mean 5-Fold mse: 0.019010426357808134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_mse_score = -cross_val_score(linear_rgr_grid_cv.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_mae_score = -cross_val_score(linear_rgr_grid_cv.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(\"Mean 5-Fold mae: {}\".format(np.mean(cv_mae_score)))\n",
    "print(\"Mean 5-Fold mse: {}\".format(np.mean(cv_mse_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 5-Fold mae: 0.061962005298446754\n",
      "Mean 5-Fold mse: 0.018781846263504084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_mse_score = -cross_val_score(rf_rgr_rand_cv.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_mae_score = -cross_val_score(rf_rgr_rand_cv.best_estimator_, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(\"Mean 5-Fold mae: {}\".format(np.mean(cv_mae_score)))\n",
    "print(\"Mean 5-Fold mse: {}\".format(np.mean(cv_mse_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest do better in terms of both metrics -> smaller error in prediction; RF as final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.056731637508977265\n",
      "Test MSE: 0.011979488269715103\n"
     ]
    }
   ],
   "source": [
    "###Use Random Forest as final model\n",
    "hp = {\n",
    "    'max_depth': 50,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 14,\n",
    "    'min_samples_split': 7,\n",
    "    'n_estimators': 450\n",
    "}\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                     ('rgr',  RandomForestRegressor(**hp))])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Test MAE: {}\".format(mean_absolute_error(y_pred,y_test)))\n",
    "print(\"Test MSE: {}\".format(mean_squared_error(y_pred,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used final model to test on test set, and yields even better results than CV on training set, which confirms the model has decent generality and ready for production.<br>\n",
    "\n",
    "In our case, performance of baseline model Ridge is very close to our final model; this is because many independent variables do have a linear relationship with response, and response variable follows normal distribution. Therefore, a linear model is suitable for this dataset. However, Random Forest still performs slightly better, as its setup is more complex and suitable for every case, but it takes much more computation power to train as well.<br>\n",
    "\n",
    "For this problem, we predicted log-error between Zillow's estimate and actual price. In my opinion, the business problem here is, if we can develop a model to predict this error, by examining our model and feature importance, we can understand how Zillow can improve their model for price prediction.<br>\n",
    "\n",
    "Next step: We understand that which factors are causing the error in Zillow's estimate, and we then improve Zillow's model to get a better estimate on top of our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
